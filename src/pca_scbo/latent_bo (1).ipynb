{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:09:17.007960Z",
     "start_time": "2025-11-03T19:09:13.767348Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — seeds + imports (run once)\n",
    "\n",
    "import os, math, warnings\n",
    "import numpy as np\n",
    "import torch, gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import botorch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.exceptions import OptimizationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=gpytorch.utils.warnings.NumericalWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=OptimizationWarning)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    import random\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    # deterministic-ish kernels\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d76d71516548744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:09:19.165987Z",
     "start_time": "2025-11-03T19:09:19.149894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2 — SpeedReducerProblem (your exact version)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SpeedReducerProblem:\n",
    "    def __init__(self, enforce_integer_teeth=True):\n",
    "        self.enforce_integer_teeth = enforce_integer_teeth\n",
    "        self.lb = np.array([2.6, 0.7, 17.0, 7.3, 7.3, 2.9, 5.0], dtype=np.float64)\n",
    "        self.ub = np.array([3.6, 0.8, 28.0, 8.3, 8.3, 3.9, 5.5], dtype=np.float64)\n",
    "        self.dim = 7\n",
    "        self.n_constraints = 11\n",
    "        self.f_star_ref = 2996.3482\n",
    "\n",
    "    def coerce_shape(self, X):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        was_1d = (X.ndim == 1)\n",
    "        if was_1d: X = X[None, :]\n",
    "        assert X.shape[-1] == 7, \"Expected last dimension to be 7.\"\n",
    "        return X, was_1d\n",
    "\n",
    "    def project_to_bounds(self, X):\n",
    "        return np.clip(X, self.lb, self.ub)\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        X, was_1d = self.coerce_shape(X)\n",
    "        X = self.project_to_bounds(X).copy()\n",
    "        if self.enforce_integer_teeth:\n",
    "            X[:, 2] = np.rint(X[:, 2])\n",
    "        x1, x2, x3, x4, x5, x6, x7 = [X[:, i] for i in range(7)]\n",
    "        f = (0.7854 * x1 * x2 ** 2 * (3.3333 * x3 ** 2 + 14.9334 * x3 - 43.0934)\n",
    "             - 1.508 * x1 * (x6 ** 2 + x7 ** 2)\n",
    "             + 7.4777 * (x6 ** 3 + x7 ** 3)\n",
    "             + 0.7854 * (x4 * x6 ** 2 + x5 * x7 ** 2))\n",
    "        g = np.empty((X.shape[0], 11), dtype=np.float64)\n",
    "        g[:, 0] = 27.0 / (x1 * x2 ** 2 * x3) - 1.0\n",
    "        g[:, 1] = 397.5 / (x1 * x2 ** 2 * x3 ** 2) - 1.0\n",
    "        g[:, 2] = 1.93 * x4 ** 3 / (x2 * x3 * x6 ** 4) - 1.0\n",
    "        g[:, 3] = 1.93 * x5 ** 3 / (x2 * x3 * x7 ** 4) - 1.0\n",
    "        g[:, 4] = np.sqrt((745.0 * x4 / (x2 * x3)) ** 2 + 16.9e6) / (0.1 * x6 ** 3) - 1100.0\n",
    "        g[:, 5] = np.sqrt((745.0 * x5 / (x2 * x3)) ** 2 + 157.5e6) / (0.1 * x7 ** 3) - 850.0\n",
    "        g[:, 6] = x2 * x3 - 40.0\n",
    "        g[:, 7] = 5.0 - x1 / x2\n",
    "        g[:, 8] = x1 / x2 - 12.0\n",
    "        g[:, 9] = 1.5 * x6 + 1.9 - x4\n",
    "        g[:, 10] = 1.1 * x7 + 1.9 - x5\n",
    "        if was_1d: return f[0], g[0]\n",
    "        return f, g\n",
    "\n",
    "    def is_feasible(self, X):\n",
    "        _, g = self.evaluate(X)\n",
    "        return np.all(g <= 0.0, axis=-1)\n",
    "\n",
    "    def sample_lhs(self, n, rng=None):\n",
    "        if rng is None: rng = np.random.default_rng()\n",
    "        d = self.dim\n",
    "        cut = np.linspace(0, 1, n + 1)\n",
    "        a, b = cut[:-1], cut[1:]\n",
    "        u = rng.random((n, d))\n",
    "        pts = a[:, None] + (b - a)[:, None] * u\n",
    "        X = np.empty_like(pts)\n",
    "        for j in range(d):\n",
    "            X[:, j] = pts[rng.permutation(n), j]\n",
    "        X = self.lb + X * (self.ub - self.lb)\n",
    "        if self.enforce_integer_teeth:\n",
    "            X[:, 2] = np.rint(X[:, 2])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "226bd6bc44e934db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:09:24.540017Z",
     "start_time": "2025-11-03T19:09:24.525241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3 — GPs + helpers (sturdy + on device)\n",
    "\n",
    "def standardize(y_np):\n",
    "    y_np = np.asarray(y_np, dtype=np.float64).reshape(-1, 1)\n",
    "    mu = float(y_np.mean())\n",
    "    sd = float(y_np.std() + 1e-12)\n",
    "    y_std = (y_np - mu) / sd\n",
    "    return y_std.ravel(), mu, sd\n",
    "\n",
    "\n",
    "class ExactGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X, y, likelihood):\n",
    "        super().__init__(X, y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(ard_num_dims=X.shape[-1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        cov = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, cov)\n",
    "\n",
    "\n",
    "def fit_gp(X_np, y_np, iters=100, lr=0.05):\n",
    "    y_std, mu, sd = standardize(y_np)\n",
    "    X_t = torch.as_tensor(X_np, dtype=torch.float64, device=device)\n",
    "    y_t = torch.as_tensor(y_std, dtype=torch.float64, device=device)\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-6))\n",
    "    model = ExactGP(X_t, y_t, likelihood).to(device)\n",
    "\n",
    "    model.train();\n",
    "    likelihood.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        opt.zero_grad()\n",
    "        with gpytorch.settings.cholesky_jitter(1e-5):\n",
    "            out = model(X_t)\n",
    "            loss = -mll(out, y_t)\n",
    "        loss.backward();\n",
    "        opt.step()\n",
    "\n",
    "    model.eval();\n",
    "    likelihood.eval()\n",
    "    model._y_mu, model._y_sd = mu, sd\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "def best_feasible_value(f_hist, C_hist):\n",
    "    feas = np.all(C_hist <= 0.0, axis=1)\n",
    "    return np.min(f_hist[feas]) if np.any(feas) else np.nan\n",
    "\n",
    "\n",
    "def least_violation_index(C_hist):\n",
    "    viol = np.clip(C_hist, 0.0, None).sum(axis=1)\n",
    "    return int(np.argmin(viol))\n",
    "\n",
    "\n",
    "def posterior_mean_std(model, X_t):\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        post = model(X_t)\n",
    "        mean = post.mean\n",
    "        std = post.variance.clamp_min(1e-12).sqrt()\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52def4a0a6763cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:09:39.972982Z",
     "start_time": "2025-11-03T19:09:39.961588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4 — TR + seeding + feasibility-first\n",
    "\n",
    "class TrustRegion:\n",
    "    def __init__(self, lb, ub, init_frac=0.8, min_frac=0.05, max_frac=1.0,\n",
    "                 grow=1.6, shrink=0.5, succ_tol=3, fail_tol=3, rng=None):\n",
    "        self.lb = lb.astype(np.float32)\n",
    "        self.ub = ub.astype(np.float32)\n",
    "        self.center = (self.lb + self.ub) / 2.0\n",
    "        self.frac = init_frac\n",
    "        self.min_frac, self.max_frac = min_frac, max_frac\n",
    "        self.grow, self.shrink = grow, shrink\n",
    "        self.succ_tol, self.fail_tol = succ_tol, fail_tol\n",
    "        self.succ = 0;\n",
    "        self.fail = 0\n",
    "        self.rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    def set_center(self, x_c):\n",
    "        self.center = x_c.astype(np.float32)\n",
    "\n",
    "    def sample(self, n_cand):\n",
    "        halfspan = 0.5 * self.frac * (self.ub - self.lb)\n",
    "        lo = np.maximum(self.center - halfspan, self.lb)\n",
    "        hi = np.minimum(self.center + halfspan, self.ub)\n",
    "        U = self.rng.random((n_cand, len(self.lb))).astype(np.float32)\n",
    "        return lo + U * (hi - lo)\n",
    "\n",
    "    def step(self, success):\n",
    "        if success:\n",
    "            self.succ += 1;\n",
    "            self.fail = 0\n",
    "            if self.succ >= self.succ_tol:\n",
    "                self.frac = min(self.max_frac, self.frac * self.grow);\n",
    "                self.succ = 0\n",
    "        else:\n",
    "            self.fail += 1;\n",
    "            self.succ = 0\n",
    "            if self.fail >= self.fail_tol:\n",
    "                self.frac = max(self.min_frac, self.frac * self.shrink);\n",
    "                self.fail = 0\n",
    "\n",
    "\n",
    "def cheap_filter(problem, n_try=200000, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    lb, ub = problem.lb, problem.ub\n",
    "    X = lb + rng.random((n_try, problem.dim)) * (ub - lb)\n",
    "    if problem.enforce_integer_teeth: X[:, 2] = np.rint(X[:, 2])\n",
    "    x1, x2, x3, x4, x5, x6, x7 = [X[:, i] for i in range(7)]\n",
    "    mask = (\n",
    "            (x2 * x3 <= 40.0) &\n",
    "            (x1 / x2 >= 5.0) &\n",
    "            (x1 / x2 <= 12.0) &\n",
    "            (1.5 * x6 + 1.9 <= x4)\n",
    "    )\n",
    "    return X[mask]\n",
    "\n",
    "\n",
    "def find_feasible_seed(problem, max_batches=5, per_batch=50000, verbose=True):\n",
    "    tried = 0\n",
    "    for b in range(max_batches):\n",
    "        cand = cheap_filter(problem, n_try=per_batch)\n",
    "        tried += per_batch\n",
    "        if cand.size == 0:\n",
    "            if verbose: print(f\"[seed] batch {b + 1}: 0 survivors\")\n",
    "            continue\n",
    "        _, G = problem.evaluate(cand)\n",
    "        feas = np.all(G <= 0.0, axis=1)\n",
    "        if np.any(feas):\n",
    "            if verbose: print(f\"[seed] found feasible after ~{tried} draws\")\n",
    "            return cand[feas][0]\n",
    "        else:\n",
    "            if verbose: print(f\"[seed] batch {b + 1}: 0 feasible among {len(cand)}\")\n",
    "    if verbose: print(\"[seed] no feasible seed found\")\n",
    "    return None\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_candidate_feasibility_first(models_c, cand_t):\n",
    "    normal = torch.distributions.Normal(0.0, 1.0)\n",
    "    log_pof = torch.zeros(cand_t.shape[0], dtype=cand_t.dtype, device=cand_t.device)\n",
    "    for mc in models_c:\n",
    "        mu, sd = posterior_mean_std(mc, cand_t)\n",
    "        z = -mu / sd\n",
    "        log_pof = log_pof + normal.log_cdf(z)\n",
    "    return torch.argmax(log_pof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45235924acf24723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:09:53.683106Z",
     "start_time": "2025-11-03T19:09:53.668960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5 — ROBUST with ModelFittingError handling\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from botorch.exceptions import ModelFittingError  # ← CRITICAL IMPORT\n",
    "import gpytorch\n",
    "from gpytorch.constraints import GreaterThan\n",
    "\n",
    "def build_models_scbo(X, f, C):\n",
    "    \"\"\"Paper-compliant SCBO with robust error handling\"\"\"\n",
    "    X_t = torch.tensor(X, dtype=torch.float64, device=device)\n",
    "    f_t = torch.tensor(f, dtype=torch.float64, device=device).reshape(-1, 1)\n",
    "    \n",
    "    likelihood_f = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=GreaterThan(1e-6)\n",
    "    )\n",
    "    \n",
    "    mf = SingleTaskGP(\n",
    "        train_X=X_t,\n",
    "        train_Y=f_t,\n",
    "        likelihood=likelihood_f,\n",
    "        covar_module=gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=X_t.shape[-1],\n",
    "                lengthscale_constraint=GreaterThan(1e-4)\n",
    "            )\n",
    "        ),\n",
    "        input_transform=Normalize(d=X_t.shape[-1]),\n",
    "        outcome_transform=Standardize(m=1)\n",
    "    )\n",
    "    mll_f = gpytorch.mlls.ExactMarginalLogLikelihood(mf.likelihood, mf)\n",
    "    \n",
    "    with gpytorch.settings.cholesky_jitter(1e-3), \\\n",
    "         gpytorch.settings.max_cholesky_size(float('inf')):\n",
    "        try:\n",
    "            fit_gpytorch_mll(mll_f, options={'maxiter': 100})\n",
    "        except (RuntimeError, gpytorch.utils.errors.NotPSDError, ModelFittingError):\n",
    "            # Use model with default hyperparameters\n",
    "            pass\n",
    "    \n",
    "    models_c = []\n",
    "    for i in range(C.shape[1]):\n",
    "        c_t = torch.tensor(C[:, i], dtype=torch.float64, device=device).reshape(-1, 1)\n",
    "        \n",
    "        likelihood_c = gpytorch.likelihoods.GaussianLikelihood(\n",
    "            noise_constraint=GreaterThan(1e-6)\n",
    "        )\n",
    "        \n",
    "        mc = SingleTaskGP(\n",
    "            train_X=X_t,\n",
    "            train_Y=c_t,\n",
    "            likelihood=likelihood_c,\n",
    "            covar_module=gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(\n",
    "                    ard_num_dims=X_t.shape[-1],\n",
    "                    lengthscale_constraint=GreaterThan(1e-4)\n",
    "                )\n",
    "            ),\n",
    "            input_transform=Normalize(d=X_t.shape[-1]),\n",
    "            outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll_c = gpytorch.mlls.ExactMarginalLogLikelihood(mc.likelihood, mc)\n",
    "        \n",
    "        with gpytorch.settings.cholesky_jitter(1e-3), \\\n",
    "             gpytorch.settings.max_cholesky_size(float('inf')):\n",
    "            try:\n",
    "                fit_gpytorch_mll(mll_c, options={'maxiter': 100})\n",
    "            except (RuntimeError, gpytorch.utils.errors.NotPSDError, ModelFittingError):\n",
    "                pass\n",
    "        \n",
    "        models_c.append(mc)\n",
    "    \n",
    "    return mf, models_c, None\n",
    "\n",
    "\n",
    "class PCAReducer:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def fit_transform(self, C):\n",
    "        Z = self.pca.fit_transform(C)\n",
    "        return Z\n",
    "\n",
    "    def transform(self, C):\n",
    "        return self.pca.transform(C)\n",
    "\n",
    "    def inverse_transform(self, Z):\n",
    "        return self.pca.inverse_transform(Z)\n",
    "\n",
    "\n",
    "class KPCAReducer:\n",
    "    def __init__(self, n_components, gamma=0.1):\n",
    "        self.kpca = KernelPCA(n_components=n_components, kernel='rbf',\n",
    "                              gamma=gamma, fit_inverse_transform=True)\n",
    "\n",
    "    def fit_transform(self, C):\n",
    "        return self.kpca.fit_transform(C)\n",
    "\n",
    "    def transform(self, C):\n",
    "        return self.kpca.transform(C)\n",
    "\n",
    "    def inverse_transform(self, Z):\n",
    "        return self.kpca.inverse_transform(Z)\n",
    "\n",
    "\n",
    "def build_models_pca(X, f, C, n_components=4):\n",
    "    \"\"\"PCA-GP SCBO with robust latent GP fitting\"\"\"\n",
    "    reducer = PCAReducer(n_components)\n",
    "    Z = reducer.fit_transform(C)\n",
    "    \n",
    "    X_t = torch.tensor(X, dtype=torch.float64, device=device)\n",
    "    f_t = torch.tensor(f, dtype=torch.float64, device=device).reshape(-1, 1)\n",
    "    \n",
    "    # Objective GP\n",
    "    likelihood_f = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=GreaterThan(1e-6)\n",
    "    )\n",
    "    \n",
    "    mf = SingleTaskGP(\n",
    "        train_X=X_t,\n",
    "        train_Y=f_t,\n",
    "        likelihood=likelihood_f,\n",
    "        covar_module=gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=X_t.shape[-1],\n",
    "                lengthscale_constraint=GreaterThan(1e-4)\n",
    "            )\n",
    "        ),\n",
    "        input_transform=Normalize(d=X_t.shape[-1]),\n",
    "        outcome_transform=Standardize(m=1)\n",
    "    )\n",
    "    mll_f = gpytorch.mlls.ExactMarginalLogLikelihood(mf.likelihood, mf)\n",
    "    \n",
    "    with gpytorch.settings.cholesky_jitter(1e-3), \\\n",
    "         gpytorch.settings.max_cholesky_size(float('inf')):\n",
    "        try:\n",
    "            fit_gpytorch_mll(mll_f, options={'maxiter': 100})\n",
    "        except (RuntimeError, gpytorch.utils.errors.NotPSDError, ModelFittingError):\n",
    "            pass\n",
    "    \n",
    "    # Latent constraint GPs with extra robustness\n",
    "    models_z = []\n",
    "    for i in range(Z.shape[1]):\n",
    "        z_t = torch.tensor(Z[:, i], dtype=torch.float64, device=device).reshape(-1, 1)\n",
    "        \n",
    "        # Check if this latent variable has near-zero variance\n",
    "        z_std = z_t.std().item()\n",
    "        if z_std < 1e-6:\n",
    "            # Skip fitting for constant/near-constant latent variables\n",
    "            # Create a dummy model that predicts constant values\n",
    "            print(f\"[PCA] Warning: Latent component {i} has very low variance ({z_std:.2e}), using dummy model\")\n",
    "            \n",
    "            # Create model but don't fit it - use defaults\n",
    "            likelihood_z = gpytorch.likelihoods.GaussianLikelihood(\n",
    "                noise_constraint=GreaterThan(1e-6)\n",
    "            )\n",
    "            mz = SingleTaskGP(\n",
    "                train_X=X_t,\n",
    "                train_Y=z_t,\n",
    "                likelihood=likelihood_z,\n",
    "                covar_module=gpytorch.kernels.ScaleKernel(\n",
    "                    gpytorch.kernels.RBFKernel(\n",
    "                        ard_num_dims=X_t.shape[-1],\n",
    "                        lengthscale_constraint=GreaterThan(1e-4)\n",
    "                    )\n",
    "                ),\n",
    "                input_transform=Normalize(d=X_t.shape[-1]),\n",
    "                outcome_transform=Standardize(m=1)\n",
    "            )\n",
    "            models_z.append(mz)\n",
    "            continue\n",
    "        \n",
    "        likelihood_z = gpytorch.likelihoods.GaussianLikelihood(\n",
    "            noise_constraint=GreaterThan(1e-6)\n",
    "        )\n",
    "        \n",
    "        mz = SingleTaskGP(\n",
    "            train_X=X_t,\n",
    "            train_Y=z_t,\n",
    "            likelihood=likelihood_z,\n",
    "            covar_module=gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(\n",
    "                    ard_num_dims=X_t.shape[-1],\n",
    "                    lengthscale_constraint=GreaterThan(1e-4)\n",
    "                )\n",
    "            ),\n",
    "            input_transform=Normalize(d=X_t.shape[-1]),\n",
    "            outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll_z = gpytorch.mlls.ExactMarginalLogLikelihood(mz.likelihood, mz)\n",
    "        \n",
    "        # Try fitting with progressively higher jitter if it fails\n",
    "        fitted = False\n",
    "        for jitter in [1e-3, 1e-2, 1e-1]:\n",
    "            with gpytorch.settings.cholesky_jitter(jitter), \\\n",
    "                 gpytorch.settings.max_cholesky_size(float('inf')):\n",
    "                try:\n",
    "                    fit_gpytorch_mll(mll_z, options={'maxiter': 50})\n",
    "                    fitted = True\n",
    "                    break\n",
    "                except (RuntimeError, gpytorch.utils.errors.NotPSDError, ModelFittingError):\n",
    "                    if jitter >= 1e-1:\n",
    "                        # Last resort failed, use model with defaults\n",
    "                        print(f\"[PCA] Warning: Failed to fit latent component {i}, using default hyperparameters\")\n",
    "                    continue\n",
    "        \n",
    "        models_z.append(mz)\n",
    "    \n",
    "    return mf, models_z, reducer\n",
    "\n",
    "\n",
    "def build_models_kpca(X, f, C, n_components=4, gamma=0.1):\n",
    "    \"\"\"kPCA-GP SCBO with robust latent GP fitting\"\"\"\n",
    "    reducer = KPCAReducer(n_components, gamma=gamma)\n",
    "    Z = reducer.fit_transform(C)\n",
    "    \n",
    "    X_t = torch.tensor(X, dtype=torch.float64, device=device)\n",
    "    f_t = torch.tensor(f, dtype=torch.float64, device=device).reshape(-1, 1)\n",
    "    \n",
    "    # Objective GP\n",
    "    likelihood_f = gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=GreaterThan(1e-6)\n",
    "    )\n",
    "    \n",
    "    mf = SingleTaskGP(\n",
    "        train_X=X_t,\n",
    "        train_Y=f_t,\n",
    "        likelihood=likelihood_f,\n",
    "        covar_module=gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=X_t.shape[-1],\n",
    "                lengthscale_constraint=GreaterThan(1e-4)\n",
    "            )\n",
    "        ),\n",
    "        input_transform=Normalize(d=X_t.shape[-1]),\n",
    "        outcome_transform=Standardize(m=1)\n",
    "    )\n",
    "    mll_f = gpytorch.mlls.ExactMarginalLogLikelihood(mf.likelihood, mf)\n",
    "    \n",
    "    with gpytorch.settings.cholesky_jitter(1e-3), \\\n",
    "         gpytorch.settings.max_cholesky_size(float('inf')):\n",
    "        try:\n",
    "            fit_gpytorch_mll(mll_f, options={'maxiter': 100})\n",
    "        except (RuntimeError, gpytorch.utils.errors.NotPSDError, ModelFittingError):\n",
    "            pass\n",
    "    \n",
    "    # Latent constraint GPs with extra robustness\n",
    "    models_z = []\n",
    "    for i in range(Z.shape[1]):\n",
    "        z_t = torch.tensor(Z[:, i], dtype=torch.float64, device=device).reshape(-1, 1)\n",
    "        \n",
    "        # Check if this latent variable has near-zero variance\n",
    "        z_std = z_t.std().item()\n",
    "        if z_std < 1e-6:\n",
    "            print(f\"[kPCA] Warning: Latent component {i} has very low variance ({z_std:.2e}), using dummy model\")\n",
    "            \n",
    "            likelihood_z = gpytorch.likelihoods.GaussianLikelihood(\n",
    "                noise_constraint=GreaterThan(1e-6)\n",
    "            )\n",
    "            mz = SingleTaskGP(\n",
    "                train_X=X_t,\n",
    "                train_Y=z_t,\n",
    "                likelihood=likelihood_z,\n",
    "                covar_module=gpytorch.kernels.ScaleKernel(\n",
    "                    gpytorch.kernels.RBFKernel(\n",
    "                        ard_num_dims=X_t.shape[-1],\n",
    "                        lengthscale_constraint=GreaterThan(1e-4)\n",
    "                    )\n",
    "                ),\n",
    "                input_transform=Normalize(d=X_t.shape[-1]),\n",
    "                outcome_transform=Standardize(m=1)\n",
    "            )\n",
    "            models_z.append(mz)\n",
    "            continue\n",
    "        \n",
    "        likelihood_z = gpytorch.likelihoods.GaussianLikelihood(\n",
    "            noise_constraint=GreaterThan(1e-6)\n",
    "        )\n",
    "        \n",
    "        mz = SingleTaskGP(\n",
    "            train_X=X_t,\n",
    "            train_Y=z_t,\n",
    "            likelihood=likelihood_z,\n",
    "            covar_module=gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(\n",
    "                    ard_num_dims=X_t.shape[-1],\n",
    "                    lengthscale_constraint=GreaterThan(1e-4)\n",
    "                )\n",
    "            ),\n",
    "            input_transform=Normalize(d=X_t.shape[-1]),\n",
    "            outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll_z = gpytorch.mlls.ExactMarginalLogLikelihood(mz.likelihood, mz)\n",
    "        \n",
    "        # Try fitting with progressively higher jitter if it fails\n",
    "        fitted = False\n",
    "        for jitter in [1e-3, 1e-2, 1e-1]:\n",
    "            with gpytorch.settings.cholesky_jitter(jitter), \\\n",
    "                 gpytorch.settings.max_cholesky_size(float('inf')):\n",
    "                try:\n",
    "                    fit_gpytorch_mll(mll_z, options={'maxiter': 50})\n",
    "                    fitted = True\n",
    "                    break\n",
    "                except (RuntimeError, gpytorch.utils.errors.NotPSDError, ModelFittingError):\n",
    "                    continue\n",
    "        \n",
    "        models_z.append(mz)\n",
    "    \n",
    "    return mf, models_z, reducer\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_candidate_cts(model_f, models_c_or_z, cand_t, reducer=None):\n",
    "    \"\"\"\n",
    "    Algorithm 1: Constrained Thompson Sampling\n",
    "    Fixed shape handling for kPCA compatibility\n",
    "    \"\"\"\n",
    "    sampler = SobolQMCNormalSampler(sample_shape=torch.Size([1]))\n",
    "    \n",
    "    # Sample objective function\n",
    "    with gpytorch.settings.fast_pred_var(), \\\n",
    "         gpytorch.settings.cholesky_jitter(1e-3):\n",
    "        f_posterior = model_f.posterior(cand_t)\n",
    "        f_samp = sampler(f_posterior)  # Shape: (1, n_cand, 1)\n",
    "        f_samp = f_samp.squeeze()  # Shape: (n_cand,) - remove ALL singleton dims\n",
    "    \n",
    "    # Sample constraints (or latent constraints)\n",
    "    c_samples = []\n",
    "    for mc in models_c_or_z:\n",
    "        with gpytorch.settings.fast_pred_var(), \\\n",
    "             gpytorch.settings.cholesky_jitter(1e-3):\n",
    "            c_posterior = mc.posterior(cand_t)\n",
    "            c_samp = sampler(c_posterior)  # Shape: (1, n_cand, 1)\n",
    "            c_samp = c_samp.squeeze()  # Shape: (n_cand,) - remove ALL singleton dims\n",
    "            \n",
    "            # Ensure 1D for stacking\n",
    "            if c_samp.ndim == 0:  # scalar case\n",
    "                c_samp = c_samp.unsqueeze(0)\n",
    "            if c_samp.ndim > 1:  # shouldn't happen but be safe\n",
    "                c_samp = c_samp.flatten()\n",
    "            \n",
    "            c_samples.append(c_samp)\n",
    "    \n",
    "    # Stack into (n_cand, g) or (n_cand, G)\n",
    "    Z_samp = torch.stack(c_samples, dim=-1)  # Shape: (n_cand, g/G)\n",
    "    \n",
    "    # Critical: Ensure 2D shape for inverse_transform\n",
    "    assert Z_samp.ndim == 2, f\"Expected 2D tensor, got shape {Z_samp.shape}\"\n",
    "    \n",
    "    # If using dimensionality reduction, map back to original space\n",
    "    if reducer is not None:\n",
    "        Z_np = Z_samp.cpu().numpy()  # Shape: (n_cand, g) as numpy array\n",
    "        \n",
    "        # kPCA inverse_transform requires 2D input: (n_samples, n_components)\n",
    "        assert Z_np.ndim == 2, f\"Z_np must be 2D for inverse_transform, got shape {Z_np.shape}\"\n",
    "        \n",
    "        C_np = reducer.inverse_transform(Z_np)  # Shape: (n_cand, G)\n",
    "        C_samp = torch.as_tensor(C_np, dtype=cand_t.dtype, device=cand_t.device)\n",
    "    else:\n",
    "        C_samp = Z_samp\n",
    "    \n",
    "    # Check feasibility in original constraint space\n",
    "    feas = torch.all(C_samp <= 0.0, dim=-1)\n",
    "    \n",
    "    if feas.any():\n",
    "        # If feasible points exist, pick argmin(objective)\n",
    "        idx = torch.argmin(f_samp[feas])\n",
    "        best_idx = torch.nonzero(feas, as_tuple=False).squeeze(1)[idx]\n",
    "    else:\n",
    "        # Otherwise, pick argmin(total_violation)\n",
    "        viol = torch.clamp(C_samp, min=0.0).sum(dim=-1)\n",
    "        best_idx = torch.argmin(viol)\n",
    "    \n",
    "    return best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1967d7e6c124ef21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:10:26.205371Z",
     "start_time": "2025-11-03T19:10:26.189043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6 — unified BO runner (no changes needed, uses updated Cell 5 functions)\n",
    "\n",
    "def run_bo(method=\"scbo\", seed=42, n_init=20, iters=100, n_cand=4096,\n",
    "           n_components=4, kpca_gamma=0.1, verbose=True,\n",
    "           enforce_integer_teeth=True, try_seed_feasible=True):\n",
    "    set_global_seed(seed)\n",
    "    problem = SpeedReducerProblem(enforce_integer_teeth=enforce_integer_teeth)\n",
    "\n",
    "    # initial DoE\n",
    "    X = problem.sample_lhs(n_init, rng=np.random.default_rng(seed))\n",
    "    f_list, C_list = [], []\n",
    "    for x in X:\n",
    "        f_i, g_i = problem.evaluate(x)\n",
    "        f_list.append(f_i)\n",
    "        C_list.append(g_i)\n",
    "    f_vals = np.array(f_list, dtype=np.float64)\n",
    "    C_vals = np.array(C_list, dtype=np.float64)\n",
    "\n",
    "    # optionally try to seed one feasible point if none\n",
    "    feas = np.all(C_vals <= 0.0, axis=1)\n",
    "    if (not np.any(feas)) and try_seed_feasible:\n",
    "        x_seed = find_feasible_seed(problem, max_batches=5, per_batch=60000, verbose=verbose)\n",
    "        if x_seed is not None:\n",
    "            f_seed, g_seed = problem.evaluate(x_seed)\n",
    "            X = np.vstack([X, x_seed])\n",
    "            f_vals = np.append(f_vals, f_seed)\n",
    "            C_vals = np.vstack([C_vals, g_seed])\n",
    "            if verbose: print(\"[seed] appended one feasible seed\")\n",
    "\n",
    "    # incumbent + TR\n",
    "    feas = np.all(C_vals <= 0.0, axis=1)\n",
    "    if np.any(feas):\n",
    "        inc = X[feas][np.argmin(f_vals[feas])]\n",
    "    else:\n",
    "        inc = X[least_violation_index(C_vals)]\n",
    "    tr = TrustRegion(problem.lb, problem.ub, init_frac=0.8)\n",
    "    tr.set_center(inc.astype(np.float32))\n",
    "\n",
    "    curr_best = inc\n",
    "    best_hist = []\n",
    "\n",
    "    for t in range(iters):\n",
    "        if verbose:\n",
    "            print(f\"{method.upper()} iter {t + 1}/{iters} | TR frac={tr.frac:.3f}\")\n",
    "\n",
    "        # build models using BoTorch-enhanced functions from Cell 5\n",
    "        if method == \"scbo\":\n",
    "            mf, mcs, reducer = build_models_scbo(X, f_vals, C_vals)\n",
    "        elif method == \"pca\":\n",
    "            mf, mcs, reducer = build_models_pca(X, f_vals, C_vals, n_components=n_components)\n",
    "        elif method == \"kpca\":\n",
    "            mf, mcs, reducer = build_models_kpca(X, f_vals, C_vals, n_components=n_components, gamma=kpca_gamma)\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'scbo', 'pca', or 'kpca'\")\n",
    "\n",
    "        # candidates (more when no feasible yet)\n",
    "        have_feas = np.any(np.all(C_vals <= 0.0, axis=1))\n",
    "        n_cand_this = n_cand if have_feas else max(n_cand, 8192)\n",
    "\n",
    "        cand_np = tr.sample(n_cand_this)\n",
    "        cand_t = torch.tensor(cand_np, dtype=torch.float64, device=device)\n",
    "        if problem.enforce_integer_teeth:\n",
    "            cand_t[:, 2] = torch.round(cand_t[:, 2])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not have_feas:\n",
    "                # feasibility-first\n",
    "                best_idx = pick_candidate_feasibility_first(mcs, cand_t)\n",
    "            else:\n",
    "                # CTS (objective + constraints) using Algorithm 1\n",
    "                best_idx = pick_candidate_cts(mf, mcs, cand_t, reducer=reducer)\n",
    "            x_next = cand_t[best_idx].cpu().numpy()\n",
    "\n",
    "        f_next, g_next = problem.evaluate(x_next)\n",
    "        X = np.vstack([X, x_next])\n",
    "        f_vals = np.append(f_vals, f_next)\n",
    "        C_vals = np.vstack([C_vals, g_next])\n",
    "\n",
    "        prev_best = best_feasible_value(f_vals[:-1], C_vals[:-1])\n",
    "        curr_best = best_feasible_value(f_vals, C_vals)\n",
    "        success = (not np.isnan(curr_best)) and (np.isnan(prev_best) or curr_best < prev_best - 1e-12)\n",
    "        tr.step(success)\n",
    "\n",
    "        feas = np.all(C_vals <= 0.0, axis=1)\n",
    "        if np.any(feas):\n",
    "            inc = X[feas][np.argmin(f_vals[feas])]\n",
    "        else:\n",
    "            inc = X[least_violation_index(C_vals)]\n",
    "        tr.set_center(inc.astype(np.float32))\n",
    "\n",
    "        best_hist.append(curr_best)\n",
    "\n",
    "    return {\n",
    "        \"X\": X, \"f\": f_vals, \"C\": C_vals, \"best_hist\": np.array(best_hist, float),\n",
    "        \"problem\": problem\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f6e6d65ee56ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T19:29:14.346513Z",
     "start_time": "2025-11-03T19:10:42.724712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCBO ===\n",
      "[seed] found feasible after ~60000 draws\n",
      "[seed] appended one feasible seed\n",
      "SCBO iter 1/8 | TR frac=0.800\n",
      "SCBO iter 2/8 | TR frac=0.800\n",
      "SCBO iter 3/8 | TR frac=0.800\n",
      "SCBO iter 4/8 | TR frac=0.800\n",
      "SCBO iter 5/8 | TR frac=0.800\n",
      "SCBO iter 6/8 | TR frac=0.800\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — run SCBO vs PCA vs kPCA (single seed) and plot\n",
    "\n",
    "seed = 12345  # <- change this to try different randomness\n",
    "n_init = 20\n",
    "iters = 100\n",
    "n_cand = 4096\n",
    "g_comp = 4  # PCA/kPCA components\n",
    "gamma = 0.2  # kPCA RBF gamma\n",
    "\n",
    "print(\"=== SCBO ===\")\n",
    "res_scbo = run_bo(\"scbo\", seed=seed, n_init=n_init, iters=iters, n_cand=n_cand)\n",
    "\n",
    "print(\"\\n=== PCA-GP SCBO ===\")\n",
    "res_pca = run_bo(\"pca\", seed=seed, n_init=n_init, iters=iters, n_cand=n_cand, n_components=g_comp)\n",
    "\n",
    "print(\"\\n=== kPCA-GP SCBO ===\")\n",
    "res_kpca = run_bo(\"kpca\", seed=seed, n_init=n_init, iters=iters, n_cand=n_cand,\n",
    "                  n_components=g_comp, kpca_gamma=gamma)\n",
    "\n",
    "\n",
    "def plot_convergence(*pairs, f_star=2996.3482):\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for name, res in pairs:\n",
    "        y = res[\"best_hist\"]\n",
    "        x = np.arange(1, len(y) + 1)\n",
    "        m = ~np.isnan(y)\n",
    "        plt.plot(x[m], y[m], lw=2, label=name)\n",
    "    plt.axhline(f_star, ls=\"--\", c=\"k\", lw=1.2, label=\"$f^*$≈{:.2f}\".format(f_star))\n",
    "    plt.xlabel(\"Evaluations\");\n",
    "    plt.ylabel(\"Best feasible f\")\n",
    "    plt.title(\"Speed Reducer — Convergence (single seed)\")\n",
    "    plt.grid(True, alpha=0.3);\n",
    "    plt.legend();\n",
    "    plt.tight_layout();\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_convergence(\n",
    "    (\"SCBO\", res_scbo),\n",
    "    (\"PCA-GP SCBO (g=4)\", res_pca),\n",
    "    (\"kPCA-GP SCBO (g=4)\", res_kpca),\n",
    "    f_star=res_scbo[\"problem\"].f_star_ref\n",
    ")\n",
    "\n",
    "print(\"Final best (SCBO):     \", np.nanmin(res_scbo[\"best_hist\"]))\n",
    "print(\"Final best (PCA-GP):   \", np.nanmin(res_pca[\"best_hist\"]))\n",
    "print(\"Final best (kPCA-GP):  \", np.nanmin(res_kpca[\"best_hist\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
